import torch
import torch.nn.functional as F
from sanguo_model import load_model_and_tokenizer

class SanguoChat:
    """ä¸‰å›½æ¼”ä¹‰é£æ ¼çš„èŠå¤©æ¨¡å‹"""
    
    def __init__(self, model_dir="../../sanguo_checkpoint"):
        print("æ­£åœ¨åŠ è½½ä¸‰å›½æ¼”ä¹‰æ¨¡å‹...")
        self.model, self.tokenizer = load_model_and_tokenizer(model_dir)
        self.model.eval()
        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        print(f"è¯æ±‡è¡¨å¤§å°: {self.tokenizer.vocab_size}")
        
    def generate_text(self, prompt, max_length=100, temperature=1.0, top_k=50):
        """ç”Ÿæˆæ–‡æœ¬"""
        # ç¼–ç è¾“å…¥
        input_ids = self.tokenizer.encode(prompt)
        if len(input_ids) == 0:
            input_ids = [self.tokenizer.char_to_idx.get('<start>', 0)]
        
        generated = input_ids.copy()
        
        with torch.no_grad():
            for _ in range(max_length):
                # å‡†å¤‡è¾“å…¥åºåˆ—
                if len(generated) >= self.model.seq_length:
                    # å¦‚æœåºåˆ—å¤ªé•¿ï¼Œå–æœ€åseq_lengthä¸ªtoken
                    input_sequence = generated[-self.model.seq_length:]
                else:
                    # å¦‚æœåºåˆ—å¤ªçŸ­ï¼Œç”¨paddingå¡«å……
                    input_sequence = generated + [0] * (self.model.seq_length - len(generated))
                
                input_tensor = torch.tensor([input_sequence])
                
                # æ¨¡å‹é¢„æµ‹
                outputs = self.model(input_tensor)
                
                # å–æœ€åä¸€ä¸ªä½ç½®çš„logitsï¼ˆå¯¹åº”ä¸‹ä¸€ä¸ªtokençš„é¢„æµ‹ï¼‰
                next_token_logits = outputs[0, min(len(generated)-1, self.model.seq_length-1), :]
                
                # åº”ç”¨temperature
                next_token_logits = next_token_logits / temperature
                
                # Top-ké‡‡æ ·
                if top_k > 0:
                    # ä¿ç•™top-kä¸ªæœ€å¯èƒ½çš„token
                    top_k_logits, top_k_indices = torch.topk(next_token_logits, min(top_k, next_token_logits.size(-1)))
                    # å°†å…¶ä»–tokençš„æ¦‚ç‡è®¾ä¸ºè´Ÿæ— ç©·
                    next_token_logits = torch.full_like(next_token_logits, float('-inf'))
                    next_token_logits.scatter_(0, top_k_indices, top_k_logits)
                
                # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
                probs = F.softmax(next_token_logits, dim=-1)
                
                # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
                next_token = torch.multinomial(probs, num_samples=1).item()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸç¬¦æˆ–ç‰¹æ®Šç¬¦å·
                if next_token == self.tokenizer.char_to_idx.get('<end>', -1):
                    break
                if next_token == 0 or next_token >= self.tokenizer.vocab_size:
                    continue
                    
                generated.append(next_token)
                
                # è§£ç å¹¶æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
                current_text = self.tokenizer.decode(generated[len(input_ids):])
                if len(current_text) > 0 and current_text[-1] in ['ã€‚', 'ï¼', 'ï¼Ÿ', '\n']:
                    break
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = self.tokenizer.decode(generated[len(input_ids):])
        return generated_text
    
    def chat(self, user_input, max_length=50, temperature=0.8, top_k=30):
        """èŠå¤©å¯¹è¯"""
        # ç”Ÿæˆå›å¤
        response = self.generate_text(user_input, max_length, temperature, top_k)
        
        # æ¸…ç†å›å¤ï¼ˆç§»é™¤å¯èƒ½çš„ä¹±ç æˆ–é‡å¤ï¼‰
        response = response.strip()
        if not response:
            response = "..."
            
        return response
    
    def interactive_chat(self):
        """äº¤äº’å¼èŠå¤©"""
        print("\n" + "="*50)
        print("ğŸ­ æ¬¢è¿ä¸ä¸‰å›½æ¼”ä¹‰é£æ ¼çš„AIå¯¹è¯ï¼")
        print("ğŸ’¡ è¯•è¯•è¯´ï¼š'å´è¯´å¤©ä¸‹å¤§åŠ¿' æˆ– 'åˆ˜å¤‡å¦‚ä½•ï¼Ÿ'")
        print("âš¡ è¾“å…¥ 'quit' é€€å‡ºå¯¹è¯")
        print("="*50)
        
        while True:
            try:
                user_input = input("\nä½ : ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                    print("\nå†ä¼šï¼æ„¿ä½ å¦‚å…³ç¾½èˆ¬ä¹‰è–„äº‘å¤©ï¼ğŸŒŸ")
                    break
                
                if not user_input:
                    continue
                
                print("\nä¸‰å›½AI: ", end="", flush=True)
                
                # ç”Ÿæˆå›å¤
                response = self.chat(user_input, max_length=80, temperature=0.8, top_k=30)
                
                # æ‰“å­—æœºæ•ˆæœæ˜¾ç¤º
                import time
                for char in response:
                    print(char, end="", flush=True)
                    time.sleep(0.05)  # è°ƒæ•´æ‰“å­—é€Ÿåº¦
                
                print()  # æ¢è¡Œ
                
            except KeyboardInterrupt:
                print("\n\nç¨‹åºè¢«ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except Exception as e:
                print(f"\nå‘ç”Ÿé”™è¯¯: {e}")
                continue

def quick_test(chat_bot):
    """å¿«é€Ÿæµ‹è¯•ç”Ÿæˆæ•ˆæœ"""
    print("\nğŸ” å¿«é€Ÿæµ‹è¯•æ¨¡å‹ç”Ÿæˆæ•ˆæœ:")
    print("-" * 40)
    
    test_prompts = [
        "å´è¯´å¤©ä¸‹å¤§åŠ¿",
        "åˆ˜å¤‡",
        "å…³ç¾½",
        "è¯¸è‘›äº®",
        "ä¸‰å›½"
    ]
    
    for prompt in test_prompts:
        response = chat_bot.chat(prompt, max_length=30, temperature=0.7)
        print(f"è¾“å…¥: '{prompt}' â†’ AI: {response}")
    
    print("-" * 40)

if __name__ == "__main__":
    try:
        import os
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(os.path.dirname(current_dir))
        model_dir = os.path.join(project_root, "sanguo_checkpoint")
        # åˆ›å»ºèŠå¤©æœºå™¨äºº - æ­£ç¡®çš„è·¯å¾„
        chat_bot = SanguoChat(model_dir)
        
        # å¿«é€Ÿæµ‹è¯•
        quick_test(chat_bot)
        
        # å¼€å§‹äº¤äº’å¼å¯¹è¯
        chat_bot.interactive_chat()
        
    except FileNotFoundError as e:
        print("âŒ æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ï¼")
        print(f"é”™è¯¯è¯¦æƒ…: {e}")
        print("è¯·ç¡®ä¿å·²ç»è®­ç»ƒå¹¶ä¿å­˜äº†æ¨¡å‹åˆ° 'sanguo_checkpoint' ç›®å½•")
        print("\nå½“å‰æŸ¥æ‰¾è·¯å¾„:")
        print("- è„šæœ¬ä½ç½®: classic/transformer_practice/")
        print("- æ¨¡å‹è·¯å¾„: ../../sanguo_checkpoint (å³é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„sanguo_checkpoint)")
    except Exception as e:
        print(f"âŒ å¯åŠ¨èŠå¤©å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´")
